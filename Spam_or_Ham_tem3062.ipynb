{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Μαυρουδής Παναγιώτης tem3062 εργστήριο3 \n",
    "- Μαζί με το παρρόν notebook επισυνάπτω ενα αρχείο που περιέχει το παραχθέν μοντέλο και ένα αρχείο με τις εκδώσεις των βιβλιοθηκών που χρησιμοποίησα.\n",
    "- Αν αντιμετωπίσετε οποιοδήποτε θέμα αναφορικά με τις βιβλιοθήκες που χρησιμοποίησα μπορείτε να χρησιμοποιήσετε την εντολή pip install -r Spam_or_Ham_requirements.txt η οποία θα κατεβάσει όλες τις βιβλιοθήκες που σας λείπουν στο συστημά σας."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys ,pickle\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imbalanced-learn in /home/thinpan/.local/lib/python3.10/site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/thinpan/.local/lib/python3.10/site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "# ! pip install imbalanced-learn\n",
    "# Download NLTK resources\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examining methods for python classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import inspect\n",
    "\n",
    "# # predictions = spam_inst.predict(test_corpus) \n",
    "# print(dir(spam_inst), \"\\n\")    \n",
    "# help(spam_inst)\n",
    "\n",
    "# # List all methods of an object\n",
    "# methods = [method for method in dir(spam_inst) if callable(getattr(spam_inst, method))]\n",
    "# # print(methods)\n",
    "\n",
    "# # Or to get more detailed information including the docstring\n",
    "# for method in methods:\n",
    "#     print(method)\n",
    "#     print(inspect.getdoc(getattr(spam_inst, method)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class spam_detection:\n",
    "    def __init__(self):\n",
    "        self.model = None \n",
    "\n",
    "        self.vectorizer_path = 'fitted_vectorizer.pkl'\n",
    "        self.vectorizer = None\n",
    "\n",
    "        self.smote = SMOTE(random_state=42)\n",
    "        \n",
    "    def load_data(self, file):\n",
    "        # Assuming the data is tab-separated and has no header\n",
    "        data = pd.read_csv(file, sep='\\t', header=None, names=['Label', 'Message'])\n",
    "        return data \n",
    "    \n",
    "    # Function to preprocess text\n",
    "    def preprocess_text(self, message):\n",
    "        \"\"\"\n",
    "        Preprocess a single SMS message by converting to lowercase, removing stopwords and non-alphanumeric characters, and tokenizing.\n",
    "\n",
    "        Parameters:\n",
    "        - message (str): The SMS message text.\n",
    "\n",
    "        Returns:\n",
    "        - str: The cleaned and preprocessed text.\n",
    "        \"\"\"\n",
    "        # Convert to lowercase\n",
    "        # sys.exit() \n",
    "        message = message.lower()\n",
    "        # Tokenize\n",
    "        words = word_tokenize(message)\n",
    "        # Remove stopwords and punctuation\n",
    "        filtered_words = [word for word in words if word not in stopwords.words('english') and word.isalnum()]\n",
    "        return ' '.join(filtered_words)\n",
    "    \n",
    "    def fit_vectorizer(self, data):\n",
    "        \"\"\"\n",
    "        Fit the vectorizer on the training data and save it to a file.\n",
    "        \n",
    "        Parameters:\n",
    "        - data (pd.Series or list): Text data to fit the vectorizer.\n",
    "\n",
    "        Returns:\n",
    "        - The fitted vectorizer instance.\n",
    "        \"\"\"\n",
    "        # Fit the vectorizer on the provided data\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.vectorizer.fit(data)\n",
    "        with open(self.vectorizer_path, 'wb') as file:\n",
    "            pickle.dump(self.vectorizer, file)\n",
    "\n",
    "    def load_vectorizer(self):\n",
    "        \"\"\"\n",
    "        Load the pre-fitted vectorizer.\n",
    "        \"\"\"\n",
    "        with open(self.vectorizer_path, 'rb') as file:\n",
    "            self.vectorizer = pickle.load(file)\n",
    "        return self.vectorizer\n",
    "\n",
    "    def vectorize_text(self, data):\n",
    "        \"\"\"\n",
    "        Convert text data into TF-IDF numeric format using a pre-fitted vectorizer loaded from a file.\n",
    "        \n",
    "        Parameters:\n",
    "        - data (pd.Series or list): Text data to transform.\n",
    "\n",
    "        Returns:\n",
    "        - scipy.sparse.csr.csr_matrix: The TF-IDF matrix of the text data.\n",
    "        \"\"\"\n",
    "        if not self.vectorizer:\n",
    "            self.load_vectorizer()\n",
    "        return self.vectorizer.transform(data)\n",
    "   \n",
    "    \n",
    "    def preprocess_pipline(self, file, fit_vectorizer=False):\n",
    "        \"\"\"\n",
    "        Preprocess the data by loading, cleaning, transforming, vectorizing the text messages, and applying SMOTE for class balancing.\n",
    "\n",
    "        This function loads data from a specified file, preprocesses the text content of the messages, maps labels from text to integers, converts the processed text messages into a sparse matrix format suitable for machine learning models, and balances the classes using SMOTE.\n",
    "\n",
    "        Parameters:\n",
    "        - file (str): The path to the file containing the dataset. The file is expected to be in a format readable by `pandas` (e.g., CSV) with at least two columns: 'Message' and 'Label'.\n",
    "        - fit_vectorizer (bool): Flag to determine if the vectorizer should be fit to the data or if an existing vectorizer should be used.\n",
    "\n",
    "        Returns:\n",
    "        - tuple:\n",
    "            - X_resampled (scipy.sparse.csr.csr_matrix or numpy.ndarray): The resampled feature matrix after applying SMOTE, potentially dense if the number of features is large.\n",
    "            - y_resampled (numpy.ndarray): The resampled label array with numerical values where 'spam' is 0 and 'ham' is 1.\n",
    "\n",
    "        The function performs the following operations:\n",
    "        1. Loads the data from the file using a custom `load_data` method.\n",
    "        2. Iterates through each message, cleans, and preprocesses it using the `preprocess_text` method.\n",
    "        3. Constructs a new DataFrame with preprocessed messages.\n",
    "        4. Resets the index of the DataFrame to ensure it starts from 0.\n",
    "        5. Maps textual labels ('spam', 'ham') to numerical values (0, 1) for model compatibility.\n",
    "        6. Vectorizes the preprocessed text messages into a sparse matrix format using the `vectorize_text` method.\n",
    "        7. Applies SMOTE to the vectorized text data to balance the classes, transforming the data matrix to handle class imbalance effectively.\n",
    "        8. Outputs diagnostic information about the processed and resampled data, including the number of non-zero values and the shape of the matrix.\n",
    "\n",
    "        Example usage:\n",
    "        >>> spam_detector = SpamDetector()  # Assuming this method is part of the SpamDetector class\n",
    "        >>> file_path = 'data/spam_data.csv'\n",
    "        >>> X_resampled, y_resampled = spam_detector.preprocess_pipline(file_path, fit_vectorizer=True)\n",
    "        \"\"\"\n",
    "        data = self.load_data(file)\n",
    "\n",
    "        messages = data[\"Message\"] \n",
    "        Filtered_messages = []\n",
    "        for message in messages:\n",
    "            filtered_words = self.preprocess_text(message)  \n",
    "        \n",
    "            # print(filtered_words) \n",
    "            Filtered_messages.append(filtered_words) \n",
    "            # print(Filtered_messages) \n",
    "\n",
    "        # Create new database with the preprocessed messages \n",
    "        data[\"Message\"] = Filtered_messages \n",
    "        # (x_train[\"Label\"] == \"ham\").sum() \n",
    "        # We have 611 spam Labels and 3846 ham messages.\n",
    "        # Assuming df is your DataFrame\n",
    "        data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # We define our mapping\n",
    "        label_mapping = {'spam': 0,'ham': 1}\n",
    "\n",
    "        # Apply the mapping to the column\n",
    "        data['Label'] = data['Label'].map(label_mapping)\n",
    "        Labels = data[\"Label\"] \n",
    "\n",
    "        if fit_vectorizer:\n",
    "            self.fit_vectorizer(data['Message'])\n",
    "\n",
    "        corpus = self.vectorize_text(data[\"Message\"]) \n",
    "        print(f\"Non zero values = {corpus.nnz}\\nShape of the Sparce matrix = {corpus.shape} \")  \n",
    "        \n",
    "        # Apply SMOTE to balance classes\n",
    "        X_resampled, y_resampled = self.smote.fit_resample(corpus, data['Label'])\n",
    "\n",
    "        return X_resampled, y_resampled\n",
    "\n",
    "    def train_model(self, features, labels):\n",
    "        \"\"\"\n",
    "        Train a Naive Bayes classifier using the provided features and labels.\n",
    "\n",
    "        Parameters:\n",
    "        - features (scipy.sparse.csr.csr_matrix or np.ndarray): The features for training.\n",
    "        - labels (pd.Series or np.ndarray): The labels for training.\n",
    "        \"\"\"\n",
    "        self.model = MultinomialNB()\n",
    "        self.model.fit(features, labels)      \n",
    "        \n",
    "    def my_predict(self, text):\n",
    "        \"\"\"Predict whether a new text is spam or ham.\n",
    "        \n",
    "        Parameters:\n",
    "        - text (str): The text message to classify.\n",
    "\n",
    "        Returns:\n",
    "        - prediction (array): Predicted label for the text.\n",
    "        \"\"\"\n",
    "        if self.model is not None:\n",
    "            # Ensure that text is in a list so it's treated as a single sample\n",
    "            features = self.vectorize_text([text])\n",
    "            return self.model.predict(features)\n",
    "        else:\n",
    "            print(\"Model has not been trained yet.\")\n",
    "            return None\n",
    "\n",
    "    def evaluate_model(self, features, labels):\n",
    "        \"\"\"\n",
    "        Evaluate the trained model using the provided test dataset and print the classification report.\n",
    "\n",
    "        Parameters:\n",
    "        - features (scipy.sparse.csr.csr_matrix or np.ndarray): The test features.\n",
    "        - labels (pd.Series or np.ndarray): The test labels.\n",
    "        \"\"\"\n",
    "        if self.model is not None:\n",
    "            predictions = self.model.predict(features)\n",
    "            print(classification_report(labels, predictions))\n",
    "        else:\n",
    "            print(\"Model is not trained or test data is not available.\")\n",
    "            \n",
    "    def save_model(self, file_name='model.pkl'):\n",
    "        \"\"\"\n",
    "        Save the fitted vectorizer and trained model to disk using pickle.\n",
    "\n",
    "        This method serializes the vectorizer and classifier model to a file, allowing them to be loaded and used later without the need for retraining. This is particularly useful for deployment or for saving model checkpoints during training. By default, the model is saved to a file named 'model.pkl', but a custom file name can be specified.\n",
    "\n",
    "        Parameters:\n",
    "        - file_name (str): The name of the file to save the model to. Default is 'model.pkl'.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump((self.vectorizer, self.model), f) \n",
    "\n",
    "\n",
    "spam_inst = spam_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train\n",
      "Non zero values = 36270\n",
      "Shape of the Sparce matrix = (4457, 7186) \n",
      "x_test\n",
      "Non zero values = 3988\n",
      "Shape of the Sparce matrix = (558, 7186) \n"
     ]
    }
   ],
   "source": [
    "x_train_file  = \"spam_train.txt\"\n",
    "x_test_file = \"spam_test.txt\"\n",
    "print(\"x_train\")\n",
    "train_corpus, train_labels = spam_inst.preprocess_pipline(x_train_file, fit_vectorizer=True)\n",
    "print(\"x_test\") \n",
    "test_corpus, test_labels = spam_inst.preprocess_pipline(x_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non zero values = 105821\n",
      "Shape of the Sparce matrix = (7692, 7186) \n",
      "Non zero values = 13484\n",
      "Shape of the Sparce matrix = (982, 7186) \n"
     ]
    }
   ],
   "source": [
    "print(f\"Non zero values = {train_corpus.nnz}\\nShape of the Sparce matrix = {train_corpus.shape} \")  \n",
    "print(f\"Non zero values = {test_corpus.nnz}\\nShape of the Sparce matrix = {test_corpus.shape} \")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 36270 elements are non zero from a total of 32.028.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model \n",
    "spam_inst.train_model(train_corpus, train_labels)     \n",
    "spam_inst.save_model()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       491\n",
      "           1       0.98      0.98      0.98       491\n",
      "\n",
      "    accuracy                           0.98       982\n",
      "   macro avg       0.98      0.98      0.98       982\n",
      "weighted avg       0.98      0.98      0.98       982\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spam_inst.evaluate_model(test_corpus, test_labels)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the Classification Report\n",
    "\n",
    "#### Basic Definitions:\n",
    "- **Precision**: The ratio of correctly predicted positive observations to the total predicted positives. It shows how relevant the model's predictions are.\n",
    "- **Recall** (Sensitivity or True Positive Rate): The ratio of correctly predicted positive observations to all actual positives. It shows how well the model can find all the positive samples.\n",
    "- **F1-Score**: The weighted average of Precision and Recall. This score takes both false positives and false negatives into account. It is especially useful when the class distribution is uneven. The formula for the F1 score is:\n",
    "  \n",
    "  \\[\n",
    "  F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "  \\]\n",
    "\n",
    "- **Support**: The number of actual occurrences of the class in the specified dataset.\n",
    "\n",
    "#### Results Explained:\n",
    "- **Class 0 (Spam, assumed) Results**:\n",
    "  - **Precision**: 1.00 - When your model predicts an instance as Class 0, it is correct every time.\n",
    "  - **Recall**: 0.79 - However, the model only successfully identifies 79% of all actual Class 0 instances.\n",
    "  - **F1-Score**: 0.88 - This is a measure of the test's accuracy. The score is relatively high, indicating a good balance between precision and recall.\n",
    "\n",
    "- **Class 1 (Ham, assumed) Results**:\n",
    "  - **Precision**: 0.97 - When your model predicts an instance as Class 1, it is correct 97% of the time.\n",
    "  - **Recall**: 1.00 - The model successfully identifies 100% of all actual Class 1 instances.\n",
    "  - **F1-Score**: 0.99 - A nearly perfect F1 score, indicating excellent precision and recall balance.\n",
    "\n",
    "#### Overall Accuracy:\n",
    "- **Accuracy**: 0.97 - Overall, the model correctly predicts the class of an instance 97% of the time across the total dataset of 558 instances.\n",
    "\n",
    "#### Averages:\n",
    "- **Macro Avg**: \n",
    "  - **Precision**: 0.99 \n",
    "  - **Recall**: 0.90 \n",
    "  - **F1-Score**: 0.93 - Macro-average will compute the metric independently for each class and then take the average (hence treating all classes equally).\n",
    "- **Weighted Avg**: \n",
    "  - **Precision**: 0.98 \n",
    "  - **Recall**: 0.97 \n",
    "  - **F1-Score**: 0.97 - Weighted average takes into account the support of each class. This is useful when dealing with class imbalance as it gives a more representative picture of the overall performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: 'Free money now!!!' - Predicted: Spam, Actual: Spam\n",
      "Message: 'ok see you there!' - Predicted: Ham, Actual: Ham\n",
      "Message: 'Congratulations prize! Clic lt ok here.' - Predicted: Spam, Actual: Spam\n"
     ]
    }
   ],
   "source": [
    "test_messages = [\"Free money now!!!\", \"ok see you there!\", \"Congratulations prize! Clic lt ok here.\"]\n",
    "test_labels = [0, 1, 0] \n",
    "\n",
    "for msg, label in zip(test_messages, test_labels):\n",
    "    prediction = spam_inst.my_predict(msg)\n",
    "    print(f\"Message: '{msg}' - Predicted: {'Spam' if prediction == 0 else 'Ham'}, Actual: {'Spam' if label == 0 else 'Ham'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to find the 5 most important for the classification process words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Word Significance in Email Classification\n",
    "\n",
    "One way to determine how indicative a word $i$ is in an email classification context is to calculate the following ratio:\n",
    "\n",
    "$$\n",
    "\\text{Odds Ratio} = \\log \\left(\\frac{P(x_j = i \\mid y = 1)}{P(x_j = i \\mid y = 0)}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $P(x_j = i \\mid y = 1)$ is the probability of word $i$ occurring in an email classified as SPAM.\n",
    "- $P(x_j = i \\mid y = 0)$ is the probability of word $i$ occurring in an email classified as NOT SPAM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob_spam = spam_inst.model.feature_log_prob_[0]  # Log probabilities of words for the spam class\n",
    "log_prob_ham = spam_inst.model.feature_log_prob_[1]   # Log probabilities of words for the non-spam class\n",
    "\n",
    "# Calculate the difference in log probabilities\n",
    "log_odds_ratio = log_prob_spam - log_prob_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = spam_inst.vectorizer.get_feature_names_out()\n",
    "\n",
    "# Find the top five most indicative words for spam\n",
    "top_spam_words_indices = np.argsort(log_odds_ratio)[-5:]  # Sort and get the top 5 indices\n",
    "top_spam_words = [(feature_names[i], log_odds_ratio[i]) for i in top_spam_words_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top indicative words for SPAM:\n",
      "awarded: 3.7793494522831006\n",
      "tone: 3.806624049179675\n",
      "guaranteed: 4.096346195280912\n",
      "prize: 4.573297002722405\n",
      "claim: 4.582111678041463\n"
     ]
    }
   ],
   "source": [
    "# Print the top five most indicative words for spam\n",
    "print(\"Top indicative words for SPAM:\")\n",
    "for word, score in top_spam_words:\n",
    "    print(f\"{word}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Indicative Words for Ham\n",
    "- **later**: 3.1176\n",
    "- **come**: 3.1273\n",
    "- **ok**: 3.1494\n",
    "- **gt**: 3.3706\n",
    "- **lt**: 3.3747\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training SVM with RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3846\n",
      "           1       1.00      1.00      1.00      3846\n",
      "\n",
      "    accuracy                           1.00      7692\n",
      "   macro avg       1.00      1.00      1.00      7692\n",
      "weighted avg       1.00      1.00      1.00      7692\n",
      "\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SVM classifier with the RBF kernel\n",
    "svm_classifier = SVC(kernel='rbf', gamma='scale')  \n",
    "svm_classifier.fit(train_corpus, train_labels)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = svm_classifier.predict(train_corpus)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(classification_report(train_labels, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(train_labels, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: '  (0, 2737)\t0.5925943429343582\n",
      "  (0, 4231)\t0.8055010519683983' - Predicted: Spam, Actual: Spam\n",
      "Message: '  (0, 3125)\t1.0' - Predicted: Ham, Actual: Ham\n",
      "Message: '  (0, 1643)\t0.6547779346195032\n",
      "  (0, 1754)\t0.6004185504149558\n",
      "  (0, 5026)\t0.45908977406714363' - Predicted: Ham, Actual: Spam\n"
     ]
    }
   ],
   "source": [
    "test_messages = [spam_inst.preprocess_text(\"Free money now!!!\"),spam_inst.preprocess_text(\"Hello\"), spam_inst.preprocess_text(\"Congratulations prize! Click here.\")]\n",
    "test_messages = spam_inst.vectorize_text(test_messages) \n",
    "test_labels = [0, 1, 0]  \n",
    "\n",
    "for msg, label in zip(test_messages, test_labels):\n",
    "\n",
    "    prediction = svm_classifier.predict(msg)\n",
    "    print(f\"Message: '{msg}' - Predicted: {'Spam' if prediction == 0 else 'Ham'}, Actual: {'Spam' if label == 0 else 'Ham'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma: 0.001, Accuracy: 0.8619344773790951\n",
      "Gamma: 0.01, Accuracy: 0.9418876755070202\n",
      "Gamma: 0.1, Accuracy: 0.9914196567862714\n",
      "Gamma: 1.0, Accuracy: 1.0\n",
      "Gamma: 10.0, Accuracy: 1.0\n",
      "Gamma: 100.0, Accuracy: 1.0\n",
      "Optimal Gamma: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Possible gamma values\n",
    "gamma_values = np.logspace(-3, 2, 6)  # Creates an array of gamma values from 0.001 to 100 on a logarithmic scale\n",
    "performance_scores = []\n",
    "\n",
    "for gamma in gamma_values:\n",
    "    svm_classifier = SVC(kernel='rbf', gamma=gamma)\n",
    "    svm_classifier.fit(train_corpus, train_labels) \n",
    "    y_pred = svm_classifier.predict(train_corpus)\n",
    "    accuracy = accuracy_score(train_labels, y_pred)\n",
    "    performance_scores.append(accuracy)\n",
    "    print(f\"Gamma: {gamma}, Accuracy: {accuracy}\")\n",
    "\n",
    "# Finding the gamma value with the highest accuracy\n",
    "optimal_gamma = gamma_values[np.argmax(performance_scores)]\n",
    "print(f\"Optimal Gamma: {optimal_gamma}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
